{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-PENLc5eu0w"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKReJ8jlesGP"
      },
      "outputs": [],
      "source": [
        "!pip install vit-pytorch\n",
        "!pip install tabulate\n",
        "!pip install perceiver-pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL4AuNm1vHAd"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "* STL-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl5F0ETMuYlN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def DataLoaders(dataset = \"CF-10\", data_root = \"./\", batch_size = 16):\n",
        "  transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p = 0.5),\n",
        "        transforms.ColorJitter(brightness=0.5, hue = 0.25),\n",
        "        transforms.ToTensor(),\n",
        "      ])\n",
        "\n",
        "  transform_test = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "  ])\n",
        "  root_dir = os.path.join(data_root, dataset)\n",
        "  if dataset == \"CF-10\":\n",
        "    train_dataset = datasets.CIFAR10(root=root_dir, train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root=root_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "  \n",
        "  if dataset == \"STL-10\":\n",
        "    train_dataset = datasets.STL10(root=root_dir, split='train', transform=transform_train, download=True)\n",
        "    test_dataset = datasets.STL10(root=root_dir, split='test', transform=transform_test, download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5ix8RAxxuAY"
      },
      "outputs": [],
      "source": [
        "def GetNumberParameters(model):\n",
        "  return sum(np.prod(p.shape).item() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dWI3UShOwDa"
      },
      "outputs": [],
      "source": [
        "def save_model(model, file_name):\n",
        "    from torch import save\n",
        "    from os import path\n",
        "    print(\"saving\", file_name)\n",
        "    return save(model.state_dict(), file_name)\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    from torch import load\n",
        "    from os import path\n",
        "    r = CNNClassifier()\n",
        "    r.load_state_dict(load(path.join(path.dirname(path.abspath(__file__)), 'cnn.th'), map_location='cpu'))\n",
        "    return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6feP4KrbQ70v"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    outputs_idx = outputs.max(1)[1].type_as(labels)\n",
        "    return outputs_idx.eq(labels).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKNCqgRwhRM",
        "outputId": "442fe24a-d728-4a97-b836-29f7d7002549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader = DataLoaders(\"STL-10\", batch_size = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ibSjJaq0Q1b"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntxAbwz3Jv3s"
      },
      "source": [
        "## Alternating Variant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CddQOIJkbi80"
      },
      "outputs": [],
      "source": [
        "from vit_pytorch import ViT\n",
        "import torchvision\n",
        "from vit_pytorch import ViT\n",
        "import torchvision.models as models\n",
        "from perceiver_pytorch import Perceiver\n",
        "\n",
        "class MMBlock(torch.nn.Module):\n",
        "  def __init__(self, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "    super().__init__()\n",
        "    self.conv_net = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 32, 15, stride = 2, padding = 2),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 32, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(128, 3, kernel_size=4, stride=3, padding=1),\n",
        "            torch.nn.BatchNorm2d(3)\n",
        "    )\n",
        "\n",
        "    self.vision_transformer = ViT(\n",
        "        image_size = image_size,\n",
        "        patch_size = image_size // 16,\n",
        "        num_classes = num_labels,\n",
        "        dim = output_dim,\n",
        "        depth = depth,\n",
        "        heads = att_heads,\n",
        "        mlp_dim = mlp_dim,\n",
        "        dropout = 0.1,\n",
        "        emb_dropout = 0.1\n",
        "    )\n",
        "    self.vision_transformer = torch.nn.Sequential(*(list(self.vision_transformer.children())[:-1]))\n",
        "\n",
        "    self.upsample_vit = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(256, 3, kernel_size=3, stride=1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(3))\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = self.conv_net(x)\n",
        "    res = self.vision_transformer(res)\n",
        "    res = res[:, :, :, None]\n",
        "    dim = int(res.shape[2] ** 0.5)\n",
        "    res = res.view(res.shape[0], res.shape[1], dim, dim)\n",
        "\n",
        "    return self.upsample_vit(res)\n",
        "\n",
        "class AlternatingMixtureModel(torch.nn.Module):\n",
        "    def __init__(self, num_blocks = 1, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "        super().__init__()\n",
        "        self.blocks =  []\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.append(MMBlock(image_size = image_size, num_labels = num_labels, depth = depth, att_heads = att_heads, mlp_dim = mlp_dim, output_dim = output_dim))\n",
        "        self.ffn = torch.nn.Linear(3 * 32 * 32, 10)\n",
        "        self.MM = torch.nn.Sequential(*self.blocks)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.ffn(self.MM(x).flatten(start_dim = 1))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AlternatingMixtureModel(2).to(device)\n",
        "img = torch.randn(16, 3, 32, 32).to(device)\n",
        "print(model(img).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT9QP5tx0cSJ"
      },
      "source": [
        "# Alternating Variant with Residual Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfZL8GZDzjat"
      },
      "outputs": [],
      "source": [
        "class ResidualCNNBlock(torch.nn.Module):\n",
        "    def __init__(self, c_in, c_out, should_stride=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if should_stride:\n",
        "            stride = 2\n",
        "        else:\n",
        "            stride = 1\n",
        "\n",
        "        self.block = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(c_in, c_out, 3, padding=1, stride=stride),\n",
        "            torch.nn.BatchNorm2d(c_out),\n",
        "        ) \n",
        "        \n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        if c_in != c_out or should_stride:\n",
        "            self.identity = torch.nn.Conv2d(c_in, c_out, 1, stride=stride)\n",
        "        else:\n",
        "            self.identity = lambda x: x\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.block(x)\n",
        "        x = self.identity(x)\n",
        "\n",
        "        return self.relu(x + result)\n",
        "\n",
        "class ResMMBlock(torch.nn.Module):\n",
        "  def __init__(self, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "    super().__init__()\n",
        "    self.conv_net = torch.nn.Sequential(\n",
        "              ResidualCNNBlock(3, 32, False),\n",
        "              ResidualCNNBlock(32, 32, False),\n",
        "              ResidualCNNBlock(32, 128, False),\n",
        "              ResidualCNNBlock(128, 128, False),\n",
        "              ResidualCNNBlock(128, 3, False)\n",
        "    )\n",
        "    self.vision_transformer = ViT(\n",
        "        image_size = image_size,\n",
        "        patch_size = image_size // 16,\n",
        "        num_classes = num_labels,\n",
        "        dim = output_dim,\n",
        "        depth = depth,\n",
        "        heads = att_heads,\n",
        "        mlp_dim = mlp_dim,\n",
        "        dropout = 0.1,\n",
        "        emb_dropout = 0.1\n",
        "    )\n",
        "    self.vision_transformer = torch.nn.Sequential(*(list(self.vision_transformer.children())[:-1]))\n",
        "\n",
        "    self.upsample_vit = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(256, 3, kernel_size=3, stride=1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(3))\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = self.conv_net(x)\n",
        "    res = self.vision_transformer(res)\n",
        "    res = res[:, :, :, None]\n",
        "    dim = int(res.shape[2] ** 0.5)\n",
        "    res = res.view(res.shape[0], res.shape[1], dim, dim)\n",
        "\n",
        "    return self.upsample_vit(res)\n",
        "\n",
        "class ResAlternatingMixtureModel(torch.nn.Module):\n",
        "    def __init__(self, num_blocks = 1, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "        super().__init__()\n",
        "        self.blocks =  []\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.append(ResMMBlock(image_size = image_size, num_labels = num_labels, depth = depth, att_heads = att_heads, mlp_dim = mlp_dim, output_dim = output_dim))\n",
        "        self.ffn = torch.nn.Linear(3 * 32 * 32, 10)\n",
        "        self.MM = torch.nn.Sequential(*self.blocks)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.ffn(self.MM(x).flatten(start_dim = 1))\n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv_net = torch.nn.Sequential(\n",
        "              ResidualCNNBlock(3, 32, False),\n",
        "              ResidualCNNBlock(32, 32, False),\n",
        "              ResidualCNNBlock(32, 128, False),\n",
        "              ResidualCNNBlock(128, 128, False),\n",
        "              ResidualCNNBlock(128, 3, False)\n",
        "    )\n",
        "    self.ffn = torch.nn.Linear(3 * 96 * 96, 10)\n",
        "\n",
        "  def forward (self, x):\n",
        "    return self.ffn(self.conv_net(x).flatten(start_dim = 1))\n",
        "\n",
        "model = ConvNet()\n",
        "print(model(torch.rand(16, 3, 96, 96)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_9AhFIObbTQ"
      },
      "source": [
        "# Alternating Variant (CNN + Perceiver)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6KvIkb_baia"
      },
      "outputs": [],
      "source": [
        "class PerceiverMMBlock(torch.nn.Module):\n",
        "  def __init__(self, image_size = 32, depth = 1, att_heads = 1, output_dim = 1024):\n",
        "    super().__init__()\n",
        "    self.conv_net = torch.nn.Sequential(\n",
        "              ResidualCNNBlock(3, 32, False),\n",
        "              ResidualCNNBlock(32, 32, False),\n",
        "              ResidualCNNBlock(32, 128, False),\n",
        "              ResidualCNNBlock(128, 128, False),\n",
        "              ResidualCNNBlock(128, 3, False)\n",
        "    )\n",
        "    self.perceiver = Perceiver(\n",
        "          input_channels = 3,          # number of channels for each token of the input\n",
        "          input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
        "          num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
        "          max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
        "          depth = depth,                   # depth of net. The shape of the final attention mechanism will be:\n",
        "                                      #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
        "          num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "          latent_dim = 32,            # latent dimension\n",
        "          cross_heads = att_heads,             # number of heads for cross attention. paper said 1\n",
        "          latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "          cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "          latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "          num_classes = output_dim,    # output number of classes\n",
        "          attn_dropout = 0.2,\n",
        "          ff_dropout = 0.2,\n",
        "          weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "          fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
        "          self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = self.conv_net(x)\n",
        "    res = res.permute(0, 2, 3, 1)\n",
        "    res = self.perceiver(res)\n",
        "    res = res[:, :, None, None]\n",
        "    res = res.view(res.shape[0], 3, 96, 96)\n",
        "\n",
        "    return res\n",
        "\n",
        "class PerceiverAltMM(torch.nn.Module):\n",
        "    def __init__(self, num_blocks = 1, image_size = 32, depth = 1, att_heads = 1, output_dim = 1024):\n",
        "        super().__init__()\n",
        "        self.blocks =  []\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.append(PerceiverMMBlock(image_size = image_size, depth = depth, att_heads = att_heads, output_dim = output_dim))\n",
        "        self.ffn = torch.nn.Linear(3 * image_size * image_size, 10)\n",
        "        self.MM = torch.nn.Sequential(*self.blocks)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.ffn(self.MM(x).flatten(start_dim = 1))\n",
        "\n",
        "model = PerceiverAltMM(num_blocks=2, image_size=96, depth = 2, att_heads=3, output_dim=3 * 96 * 96)\n",
        "print(model(torch.rand(16, 3, 96, 96)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP-JlgsnXp_0"
      },
      "source": [
        "# Joint Variant (CNN + Perceiver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNOra09WXuKZ",
        "outputId": "25398ae0-d90b-4f34-afd3-5150faa393b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 10])\n"
          ]
        }
      ],
      "source": [
        "class JointPerceiverMM(torch.nn.Module):\n",
        "    def __init__(self, num_blocks = 1, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "      super().__init__()\n",
        "      self.blocks = []\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      self.conv_net = torch.nn.Sequential(\n",
        "          ResidualCNNBlock(3, 32, False),\n",
        "          ResidualCNNBlock(32, 32, False),\n",
        "          ResidualCNNBlock(32, 128, False),\n",
        "          ResidualCNNBlock(128, 128, False),\n",
        "          ResidualCNNBlock(128, 3, False)\n",
        "      )\n",
        "      self.perceiver = Perceiver(\n",
        "        input_channels = 3,          # number of channels for each token of the input\n",
        "        input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
        "        num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
        "        max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
        "        depth = depth,                   # depth of net. The shape of the final attention mechanism will be:\n",
        "                                    #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
        "        num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "        latent_dim = 32,            # latent dimension\n",
        "        cross_heads = att_heads,             # number of heads for cross attention. paper said 1\n",
        "        latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "        cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "        latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "        num_classes = 3 * 96 * 96,    # output number of classes\n",
        "        attn_dropout = 0.2,\n",
        "        ff_dropout = 0.2,\n",
        "        weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "        fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
        "        self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
        "      )   \n",
        "\n",
        "      self.conv_net = self.conv_net.to(device)\n",
        "      self.perceiver = self.perceiver.to(device)\n",
        "\n",
        "      if num_blocks == 2:\n",
        "        self.conv_net2 = torch.nn.Sequential(\n",
        "            ResidualCNNBlock(3, 32, False),\n",
        "            ResidualCNNBlock(32, 32, False),\n",
        "            ResidualCNNBlock(32, 128, False),\n",
        "            ResidualCNNBlock(128, 128, False),\n",
        "            ResidualCNNBlock(128, 3, False)\n",
        "        )\n",
        "        self.perceiver2 = Perceiver(\n",
        "          input_channels = 3,          # number of channels for each token of the input\n",
        "          input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
        "          num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
        "          max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
        "          depth = depth,                   # depth of net. The shape of the final attention mechanism will be:\n",
        "                                      #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
        "          num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "          latent_dim = 32,            # latent dimension\n",
        "          cross_heads = att_heads,             # number of heads for cross attention. paper said 1\n",
        "          latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "          cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "          latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "          num_classes = 3 * 96 * 96,    # output number of classes\n",
        "          attn_dropout = 0.2,\n",
        "          ff_dropout = 0.2,\n",
        "          weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "          fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
        "          self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
        "        )   \n",
        "\n",
        "        self.conv_net2 = self.conv_net2.to(device)\n",
        "        self.perceiver2 = self.perceiver2.to(device)\n",
        "\n",
        "      self.ffn = torch.nn.Linear(3 * 96 * 96, 10)\n",
        "      self.norm = torch.nn.BatchNorm2d(3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        res1 = self.conv_net(input)\n",
        "        res2 = input.permute(0, 2, 3, 1)\n",
        "        res2 = self.perceiver(res2)\n",
        "        res2 = res2[:, :, None, None]\n",
        "        res2 = res2.view(res2.shape[0], 3, 96, 96)\n",
        "\n",
        "        input = self.norm(res1) + self.norm(res2)\n",
        "\n",
        "        res1 = self.conv_net2(input)\n",
        "        res2 = input.permute(0, 2, 3, 1)\n",
        "        res2 = self.perceiver2(res2)\n",
        "        res2 = res2[:, :, None, None]\n",
        "        res2 = res2.view(res2.shape[0], 3, 96, 96)\n",
        "\n",
        "        input = self.norm(res1) + self.norm(res2)\n",
        "        \n",
        "        return self.ffn(input.flatten(start_dim = 1))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = JointPerceiverMM(2).to(device)\n",
        "print(model(torch.rand(16, 3, 96, 96).to(device)).shape)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhxmtnEdJz1t"
      },
      "source": [
        "## Joint Variant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbq_fS0JJ3Hu",
        "outputId": "6dfcb6ff-341b-4372-a7d7-3bd534c7d1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JointMixtureModel(\n",
            "  (ffn): Linear(in_features=3072, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30730"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class JointMixtureModel(torch.nn.Module):\n",
        "    def __init__(self, num_blocks = 1, image_size = 32, num_labels = 10, depth = 1, att_heads = 1, mlp_dim = 2048, output_dim = 1024):\n",
        "      super().__init__()\n",
        "      self.blocks = []\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      for i in range(num_blocks):\n",
        "        conv_net = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, image_size, 15, stride = 2, padding = 2),\n",
        "            torch.nn.BatchNorm2d(image_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(image_size, image_size, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(image_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(image_size, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(128, 3, kernel_size=4, stride=3, padding=1),\n",
        "            torch.nn.BatchNorm2d(3)\n",
        "        )\n",
        "        vit = ViT(\n",
        "          image_size = image_size,\n",
        "          patch_size = image_size // 16,\n",
        "          num_classes = num_labels,\n",
        "          dim = output_dim,\n",
        "          depth = depth,\n",
        "          heads = att_heads,\n",
        "          mlp_dim = mlp_dim,\n",
        "          dropout = 0.1,\n",
        "          emb_dropout = 0.1\n",
        "        )\n",
        "        vit = torch.nn.Sequential(*(list(vit.children())[:-1]))\n",
        "\n",
        "        upsample_vit = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(256, 3, kernel_size=3, stride=1, padding = 1),\n",
        "            torch.nn.BatchNorm2d(3))\n",
        "        conv_net = conv_net.to(device)\n",
        "        vit = vit.to(device)\n",
        "        upsample_vit = upsample_vit.to(device)\n",
        "        self.blocks.append([conv_net, vit, upsample_vit])\n",
        "\n",
        "      self.ffn = torch.nn.Linear(3 * 32 * 32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        for conv_net, vit, upsample in self.blocks:\n",
        "          res1 = conv_net(input)\n",
        "          res2 = vit(input)\n",
        "          res2 = res2[:, :, :, None]\n",
        "          dim = int(res2.shape[2] ** 0.5)\n",
        "          res2 = res2.view(res2.shape[0], res2.shape[1], dim, dim)\n",
        "          res2 = upsample(res2)\n",
        "\n",
        "          input = res1 + res2\n",
        "        \n",
        "        return self.ffn(input.flatten(start_dim = 1))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = JointMixtureModel(2).to(device)\n",
        "# img = torch.randn(16, 3, 32, 32).to(device)\n",
        "# print(model(img).shape)\n",
        "print(model)\n",
        "GetNumberParameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqPvlEQcuPBV"
      },
      "source": [
        "# Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7aG6tNbMqnr"
      },
      "outputs": [],
      "source": [
        "lr = 3e-4\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "num_workers = 2\n",
        "weight_decay = 1e-4\n",
        "# Set up the cuda\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = PerceiverAltMM(num_blocks=2, image_size=96, depth = 4, att_heads=2, output_dim=3 * 96 * 96)\n",
        "# print(model)\n",
        "# model = Perceiver(\n",
        "#           input_channels = 3,          # number of channels for each token of the input\n",
        "#           input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
        "#           num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
        "#           max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
        "#           depth = 4,                   # depth of net. The shape of the final attention mechanism will be:\n",
        "#                                       #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
        "#           num_latents = 64,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
        "#           latent_dim = 64,            # latent dimension\n",
        "#           cross_heads = 8,             # number of heads for cross attention. paper said 1\n",
        "#           latent_heads = 8,            # number of heads for latent self attention, 8\n",
        "#           cross_dim_head = 16,         # number of dimensions per cross attention head\n",
        "#           latent_dim_head = 16,        # number of dimensions per latent self attention head\n",
        "#           num_classes = 10,    # output number of classes\n",
        "#           attn_dropout = 0.2,\n",
        "#           ff_dropout = 0.2,\n",
        "#           weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
        "#           fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
        "#           self_per_cross_attn = 4      # number of self attention blocks per cross attention\n",
        "#     )\n",
        "\n",
        "# model = ConvNet()\n",
        "model = JointPerceiverMM(2)\n",
        "print(GetNumberParameters(model))\n",
        "# model.load_state_dict(torch.load('det128.th', map_location='cpu'))\n",
        "# model = models.resnet18(pretrained=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "# Set up training data and validation data\n",
        "data_train, data_val = DataLoaders(\"STL-10\", batch_size = batch_size)\n",
        "\n",
        "# Wrap in a progress bar.\n",
        "for epoch in range(epochs):\n",
        "    print(\"EPOCH {}\".format(epoch))\n",
        "    # Set the model to training mode.\n",
        "    model.train()\n",
        "\n",
        "    train_accuracy_val = list()\n",
        "    for x, y in tqdm(data_train):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # x = x.permute(0, 2, 3, 1)\n",
        "        y_pred = model(x)\n",
        "        train_accuracy_val.append(accuracy(y_pred, y))\n",
        "\n",
        "        # Compute loss and update model weights.\n",
        "        loss = loss_func(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        # Add loss to TensorBoard.\n",
        "        # train_logger.add_scalar('Loss', loss.item(), global_step=global_step)\n",
        "        # global_step += 1\n",
        "\n",
        "    train_accuracy_total = torch.FloatTensor(train_accuracy_val).mean().item()\n",
        "    # train_logger.add_scalar('Train Accuracy', train_accuracy_total, global_step=global_step)\n",
        "    print(\"Train Accuracy: {:.4f}\".format(train_accuracy_total))\n",
        "\n",
        "    if epoch % 4 == 0:\n",
        "      save_model(model, 'det128_' + str(epoch) + '.th')\n",
        "\n",
        "    # Set the model to eval mode and compute accuracy.\n",
        "    model.eval()\n",
        "\n",
        "    accuracys_val = list()\n",
        "    torch.cuda.empty_cache()\n",
        "    for x, y in data_val:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)  \n",
        "        # x = x.permute(0, 2, 3, 1)        \n",
        "        y_pred = model(x)\n",
        "        accuracys_val.append(accuracy(y_pred, y))\n",
        " \n",
        "    accuracy_total = torch.FloatTensor(accuracys_val).mean().item()\n",
        "    print(\"Validation Accuracy: {:.4f}\".format(accuracy_total))\n",
        "    # valid_logger.add_scalar('Validation Accuracy', accuracy_total, global_step=global_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtyXoBfd27w"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NgeZF_OtQlpx"
      },
      "outputs": [],
      "source": [
        "#1693278 - 100 min\n",
        "temp = \"Train Accuracy: 0.1889;Validation Accuracy: 0.2278;Train Accuracy: 0.2466;Validation Accuracy: 0.2759;Train Accuracy: 0.2684;Validation Accuracy: 0.2755;Train Accuracy: 0.2875;Validation Accuracy: 0.2833;Train Accuracy: 0.3193;Validation Accuracy: 0.3279;Train Accuracy: 0.3442;Validation Accuracy: 0.4025;Train Accuracy: 0.3828;Validation Accuracy: 0.3336;Train Accuracy: 0.3986;Validation Accuracy: 0.4116;Train Accuracy: 0.4269;Validation Accuracy: 0.4956;Train Accuracy: 0.4599;Validation Accuracy: 0.4837;Train Accuracy: 0.4722;Validation Accuracy: 0.4760;Train Accuracy: 0.4782;Validation Accuracy: 0.5289;Train Accuracy: 0.4942;Validation Accuracy: 0.4901;Train Accuracy: 0.4998;Validation Accuracy: 0.5336;Train Accuracy: 0.5234;Validation Accuracy: 0.5013;Train Accuracy: 0.5152;Validation Accuracy: 0.5239;Train Accuracy: 0.5224;Validation Accuracy: 0.5260;Train Accuracy: 0.5341;Validation Accuracy: 0.5592;Train Accuracy: 0.5321;Validation Accuracy: 0.5475;Train Accuracy: 0.5427;Validation Accuracy: 0.5446;Train Accuracy: 0.5405;Validation Accuracy: 0.5644;Train Accuracy: 0.5529;Validation Accuracy: 0.5362;Train Accuracy: 0.5547;Validation Accuracy: 0.5681;Train Accuracy: 0.5627;Validation Accuracy: 0.5403;Train Accuracy: 0.5765;Validation Accuracy: 0.5779;Train Accuracy: 0.5683;Validation Accuracy: 0.5667;Train Accuracy: 0.5855;Validation Accuracy: 0.5860;Train Accuracy: 0.5731;Validation Accuracy: 0.5767;Train Accuracy: 0.5865;Validation Accuracy: 0.5539;Train Accuracy: 0.5875;Validation Accuracy: 0.5523;Train Accuracy: 0.5940;Validation Accuracy: 0.5702;Train Accuracy: 0.5970;Validation Accuracy: 0.5782;Train Accuracy: 0.6064;Validation Accuracy: 0.6079;Train Accuracy: 0.6126;Validation Accuracy: 0.5454;Train Accuracy: 0.6142;Validation Accuracy: 0.6083;Train Accuracy: 0.6140;Validation Accuracy: 0.5940;Train Accuracy: 0.6164;Validation Accuracy: 0.5803;Train Accuracy: 0.6272;Validation Accuracy: 0.6080;Train Accuracy: 0.6336;Validation Accuracy: 0.6131;Train Accuracy: 0.6290;Validation Accuracy: 0.6173;Train Accuracy: 0.6370;Validation Accuracy: 0.6069;Train Accuracy: 0.6368;Validation Accuracy: 0.6036;Train Accuracy: 0.6468;Validation Accuracy: 0.5626;Train Accuracy: 0.6583;Validation Accuracy: 0.5698;Train Accuracy: 0.6454;Validation Accuracy: 0.5849;Train Accuracy: 0.6444;Validation Accuracy: 0.6110;Train Accuracy: 0.6556;Validation Accuracy: 0.6438;Train Accuracy: 0.6623;Validation Accuracy: 0.6259;Train Accuracy: 0.6641;Validation Accuracy: 0.5885;Train Accuracy: 0.6611;Validation Accuracy: 0.6080\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_alt = [\"Alternating 1 Block\"]\n",
        "val_alt = [\"Alternating 1 Block\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_alt.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_alt.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "e6EBj0MIVrov"
      },
      "outputs": [],
      "source": [
        "# Perceiver - 50 min, 301170\n",
        "temp = \"Train Accuracy: 0.1162;Validation Accuracy: 0.1119;Train Accuracy: 0.1693;Validation Accuracy: 0.2457;Train Accuracy: 0.2226;Validation Accuracy: 0.2404;Train Accuracy: 0.2392;Validation Accuracy: 0.2785;Train Accuracy: 0.2600;Validation Accuracy: 0.2928;Train Accuracy: 0.2644;Validation Accuracy: 0.2934;Train Accuracy: 0.2835;Validation Accuracy: 0.2810;Train Accuracy: 0.2961;Validation Accuracy: 0.3039;Train Accuracy: 0.2945;Validation Accuracy: 0.3154;Train Accuracy: 0.2979;Validation Accuracy: 0.3252;Train Accuracy: 0.3133;Validation Accuracy: 0.3161;Train Accuracy: 0.3139;Validation Accuracy: 0.3261;Train Accuracy: 0.3207;Validation Accuracy: 0.3360;Train Accuracy: 0.3229;Validation Accuracy: 0.3327;Train Accuracy: 0.3319;Validation Accuracy: 0.3442;Train Accuracy: 0.3399;Validation Accuracy: 0.3405;Train Accuracy: 0.3460;Validation Accuracy: 0.3579;Train Accuracy: 0.3510;Validation Accuracy: 0.3569;Train Accuracy: 0.3586;Validation Accuracy: 0.3696;Train Accuracy: 0.3640;Validation Accuracy: 0.3778;Train Accuracy: 0.3712;Validation Accuracy: 0.3750;Train Accuracy: 0.3764;Validation Accuracy: 0.3923;Train Accuracy: 0.3870;Validation Accuracy: 0.3832;Train Accuracy: 0.3790;Validation Accuracy: 0.3900;Train Accuracy: 0.3896;Validation Accuracy: 0.3909;Train Accuracy: 0.3954;Validation Accuracy: 0.3944;Train Accuracy: 0.3976;Validation Accuracy: 0.3902;Train Accuracy: 0.3996;Validation Accuracy: 0.4026;Train Accuracy: 0.4004;Validation Accuracy: 0.4085;Train Accuracy: 0.4050;Validation Accuracy: 0.4002;Train Accuracy: 0.4109;Validation Accuracy: 0.4106;Train Accuracy: 0.4093;Validation Accuracy: 0.4008;Train Accuracy: 0.4261;Validation Accuracy: 0.4145;Train Accuracy: 0.4205;Validation Accuracy: 0.4091;Train Accuracy: 0.4223;Validation Accuracy: 0.4025;Train Accuracy: 0.4227;Validation Accuracy: 0.4157;Train Accuracy: 0.4265;Validation Accuracy: 0.4083;Train Accuracy: 0.4283;Validation Accuracy: 0.4115;Train Accuracy: 0.4271;Validation Accuracy: 0.4141;Train Accuracy: 0.4313;Validation Accuracy: 0.4186;Train Accuracy: 0.4343;Validation Accuracy: 0.4185;Train Accuracy: 0.4341;Validation Accuracy: 0.4162;Train Accuracy: 0.4379;Validation Accuracy: 0.4202;Train Accuracy: 0.4395;Validation Accuracy: 0.4175;Train Accuracy: 0.4475;Validation Accuracy: 0.4206;Train Accuracy: 0.4485;Validation Accuracy: 0.4310;Train Accuracy: 0.4445;Validation Accuracy: 0.4129;Train Accuracy: 0.4505;Validation Accuracy: 0.4295;Train Accuracy: 0.4483;Validation Accuracy: 0.4243;Train Accuracy: 0.4503;Validation Accuracy: 0.4286\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_perceiver = [\"Perceiver\"]\n",
        "val_perceiver = [\"Perceiver\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_perceiver.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_perceiver.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-XTaTG5OZEX-"
      },
      "outputs": [],
      "source": [
        "# CNN - 33 Minutes 480054\n",
        "temp = \"Train Accuracy: 0.2588;Validation Accuracy: 0.3828;Train Accuracy: 0.4679;Validation Accuracy: 0.4499;Train Accuracy: 0.6234;Validation Accuracy: 0.4574;Train Accuracy: 0.7298;Validation Accuracy: 0.4535;Train Accuracy: 0.8053;Validation Accuracy: 0.4499;Train Accuracy: 0.8776;Validation Accuracy: 0.4355;Train Accuracy: 0.9119;Validation Accuracy: 0.4451;Train Accuracy: 0.9371;Validation Accuracy: 0.4471;Train Accuracy: 0.9631;Validation Accuracy: 0.4416;Train Accuracy: 0.9655;Validation Accuracy: 0.4419;Train Accuracy: 0.9692;Validation Accuracy: 0.4256;Train Accuracy: 0.9746;Validation Accuracy: 0.4286;Train Accuracy: 0.9766;Validation Accuracy: 0.3996;Train Accuracy: 0.9800;Validation Accuracy: 0.4179;Train Accuracy: 0.9858;Validation Accuracy: 0.4206;Train Accuracy: 0.9824;Validation Accuracy: 0.4184;Train Accuracy: 0.9796;Validation Accuracy: 0.4288;Train Accuracy: 0.9848;Validation Accuracy: 0.4136;Train Accuracy: 0.9818;Validation Accuracy: 0.4219;Train Accuracy: 0.9884;Validation Accuracy: 0.4087;Train Accuracy: 0.9878;Validation Accuracy: 0.4080;Train Accuracy: 0.9872;Validation Accuracy: 0.4212;Train Accuracy: 0.9854;Validation Accuracy: 0.4210;Train Accuracy: 0.9894;Validation Accuracy: 0.4264;Train Accuracy: 0.9912;Validation Accuracy: 0.4205;Train Accuracy: 0.9888;Validation Accuracy: 0.4103;Train Accuracy: 0.9870;Validation Accuracy: 0.4134;Train Accuracy: 0.9894;Validation Accuracy: 0.4106;Train Accuracy: 0.9894;Validation Accuracy: 0.4119;Train Accuracy: 0.9896;Validation Accuracy: 0.4185;Train Accuracy: 0.9932;Validation Accuracy: 0.4135;Train Accuracy: 0.9938;Validation Accuracy: 0.4204;Train Accuracy: 0.9890;Validation Accuracy: 0.4084;Train Accuracy: 0.9904;Validation Accuracy: 0.4055;Train Accuracy: 0.9928;Validation Accuracy: 0.4169;Train Accuracy: 0.9954;Validation Accuracy: 0.4149;Train Accuracy: 0.9932;Validation Accuracy: 0.4156;Train Accuracy: 0.9944;Validation Accuracy: 0.4004;Train Accuracy: 0.9898;Validation Accuracy: 0.4176;Train Accuracy: 0.9918;Validation Accuracy: 0.4139;Train Accuracy: 0.9878;Validation Accuracy: 0.4042;Train Accuracy: 0.9902;Validation Accuracy: 0.4059;Train Accuracy: 0.9912;Validation Accuracy: 0.4083;Train Accuracy: 0.9966;Validation Accuracy: 0.4126;Train Accuracy: 0.9944;Validation Accuracy: 0.4157;Train Accuracy: 0.9956;Validation Accuracy: 0.4149;Train Accuracy: 0.9980;Validation Accuracy: 0.4196;Train Accuracy: 0.9936;Validation Accuracy: 0.4209;Train Accuracy: 0.9928;Validation Accuracy: 0.4002;Train Accuracy: 0.9932;Validation Accuracy: 0.4119\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_cnn = [\"CNN\"]\n",
        "val_cnn = [\"CNN\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_cnn.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_cnn.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XeRlUAe_aR3f"
      },
      "outputs": [],
      "source": [
        "# Baseline (Perceiver): 125 Minutes 1625586\n",
        "temp = \"Train Accuracy: 0.1194;Validation Accuracy: 0.1785;Train Accuracy: 0.1771;Validation Accuracy: 0.2262;Train Accuracy: 0.2234;Validation Accuracy: 0.2244;Train Accuracy: 0.2442;Validation Accuracy: 0.2574;Train Accuracy: 0.2486;Validation Accuracy: 0.2747;Train Accuracy: 0.2652;Validation Accuracy: 0.2784;Train Accuracy: 0.2604;Validation Accuracy: 0.2955;Train Accuracy: 0.2792;Validation Accuracy: 0.2896;Train Accuracy: 0.2760;Validation Accuracy: 0.2959;Train Accuracy: 0.2907;Validation Accuracy: 0.2916;Train Accuracy: 0.2905;Validation Accuracy: 0.3225;Train Accuracy: 0.2983;Validation Accuracy: 0.3145;Train Accuracy: 0.3105;Validation Accuracy: 0.3061;Train Accuracy: 0.3033;Validation Accuracy: 0.3239;Train Accuracy: 0.3061;Validation Accuracy: 0.3160;Train Accuracy: 0.3097;Validation Accuracy: 0.3296;Train Accuracy: 0.3225;Validation Accuracy: 0.3229;Train Accuracy: 0.3169;Validation Accuracy: 0.3259;Train Accuracy: 0.3233;Validation Accuracy: 0.3331;Train Accuracy: 0.3277;Validation Accuracy: 0.3363;Train Accuracy: 0.3365;Validation Accuracy: 0.3483;Train Accuracy: 0.3373;Validation Accuracy: 0.3426;Train Accuracy: 0.3440;Validation Accuracy: 0.3646;Train Accuracy: 0.3506;Validation Accuracy: 0.3627;Train Accuracy: 0.3644;Validation Accuracy: 0.3663;Train Accuracy: 0.3544;Validation Accuracy: 0.3724;Train Accuracy: 0.3580;Validation Accuracy: 0.3634;Train Accuracy: 0.3572;Validation Accuracy: 0.3685;Train Accuracy: 0.3746;Validation Accuracy: 0.3735;Train Accuracy: 0.3632;Validation Accuracy: 0.3643;Train Accuracy: 0.3690;Validation Accuracy: 0.3770;Train Accuracy: 0.3774;Validation Accuracy: 0.3573;Train Accuracy: 0.3796;Validation Accuracy: 0.3811;Train Accuracy: 0.3874;Validation Accuracy: 0.3824;Train Accuracy: 0.3736;Validation Accuracy: 0.3672;Train Accuracy: 0.3776;Validation Accuracy: 0.3823;Train Accuracy: 0.3808;Validation Accuracy: 0.3949;Train Accuracy: 0.3928;Validation Accuracy: 0.3915;Train Accuracy: 0.3850;Validation Accuracy: 0.3925;Train Accuracy: 0.3972;Validation Accuracy: 0.3873;Train Accuracy: 0.3922;Validation Accuracy: 0.3988;Train Accuracy: 0.3934;Validation Accuracy: 0.3849;Train Accuracy: 0.3914;Validation Accuracy: 0.3898;Train Accuracy: 0.4000;Validation Accuracy: 0.3991;Train Accuracy: 0.4077;Validation Accuracy: 0.4017;Train Accuracy: 0.4085;Validation Accuracy: 0.4040;Train Accuracy: 0.4127;Validation Accuracy: 0.4039\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_baseline = [\"Baseline\"]\n",
        "val_baseline = [\"Baseline\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_baseline.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_baseline.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rCczWhv23sqQ"
      },
      "outputs": [],
      "source": [
        "# Alt 2 Block: 300 Minutes 3110066\n",
        "temp = \"Train Accuracy: 0.2993;Validation Accuracy: 0.3063;Train Accuracy: 0.3001;Validation Accuracy: 0.3268;Train Accuracy: 0.3259;Validation Accuracy: 0.3226;Train Accuracy: 0.3181;Validation Accuracy: 0.3147;Train Accuracy: 0.3363;Validation Accuracy: 0.3500;Train Accuracy: 0.3526;Validation Accuracy: 0.3749;Train Accuracy: 0.3622;Validation Accuracy: 0.4010;Train Accuracy: 0.3780;Validation Accuracy: 0.3844;Train Accuracy: 0.3736;Validation Accuracy: 0.3483;Train Accuracy: 0.4054;Validation Accuracy: 0.3985;Train Accuracy: 0.4063;Validation Accuracy: 0.4190;Train Accuracy: 0.4153;Validation Accuracy: 0.3831;Train Accuracy: 0.4269;Validation Accuracy: 0.4034;Train Accuracy: 0.4159;Validation Accuracy: 0.4518;Train Accuracy: 0.4337;Validation Accuracy: 0.4354;Train Accuracy: 0.4485;Validation Accuracy: 0.4504;Train Accuracy: 0.4485;Validation Accuracy: 0.4288;Train Accuracy: 0.4609;Validation Accuracy: 0.4270;Train Accuracy: 0.4734;Validation Accuracy: 0.4796;Train Accuracy: 0.4694;Validation Accuracy: 0.4734;Train Accuracy: 0.4916;Validation Accuracy: 0.4807;Train Accuracy: 0.4960;Validation Accuracy: 0.5004;Train Accuracy: 0.5070;Validation Accuracy: 0.5321;Train Accuracy: 0.5288;Validation Accuracy: 0.4991;Train Accuracy: 0.5321;Validation Accuracy: 0.5071;Train Accuracy: 0.5355;Validation Accuracy: 0.5185;Train Accuracy: 0.5294;Validation Accuracy: 0.5496;Train Accuracy: 0.5483;Validation Accuracy: 0.5163;Train Accuracy: 0.5491;Validation Accuracy: 0.5374;Train Accuracy: 0.5565;Validation Accuracy: 0.5586;Train Accuracy: 0.5679;Validation Accuracy: 0.5677;Train Accuracy: 0.5773;Validation Accuracy: 0.5794;Train Accuracy: 0.5815;Validation Accuracy: 0.5576;Train Accuracy: 0.5835;Validation Accuracy: 0.5400;Train Accuracy: 0.5901;Validation Accuracy: 0.5691;Train Accuracy: 0.6052;Validation Accuracy: 0.5232;Train Accuracy: 0.6032;Validation Accuracy: 0.5698;Train Accuracy: 0.6170;Validation Accuracy: 0.5804;Train Accuracy: 0.6244;Validation Accuracy: 0.5805;Train Accuracy: 0.6250;Validation Accuracy: 0.5829;Train Accuracy: 0.6148;Validation Accuracy: 0.5058;Train Accuracy: 0.6212;Validation Accuracy: 0.6030;Train Accuracy: 0.6310;Validation Accuracy: 0.5991;Train Accuracy: 0.6358;Validation Accuracy: 0.5136;Train Accuracy: 0.6294;Validation Accuracy: 0.5370;Train Accuracy: 0.6372;Validation Accuracy: 0.5929;Train Accuracy: 0.6458;Validation Accuracy: 0.6146;Train Accuracy: 0.6494;Validation Accuracy: 0.5811;Train Accuracy: 0.6396;Validation Accuracy: 0.6055;Train Accuracy: 0.6595;Validation Accuracy: 0.5738\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_2 = [\"Alternating - 2 Block\"]\n",
        "val_2 = [\"Alternating - 2 Block\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_2.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_2.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yCLx5Eg2AInm"
      },
      "outputs": [],
      "source": [
        "# Joint Block - 1466518, 30 min\n",
        "temp = \"Train Accuracy: 0.2823;Validation Accuracy: 0.3825;Train Accuracy: 0.4523;Validation Accuracy: 0.4565;Train Accuracy: 0.5769;Validation Accuracy: 0.4391;Train Accuracy: 0.6765;Validation Accuracy: 0.4726;Train Accuracy: 0.7438;Validation Accuracy: 0.4891;Train Accuracy: 0.7959;Validation Accuracy: 0.4699;Train Accuracy: 0.8295;Validation Accuracy: 0.4629;Train Accuracy: 0.8564;Validation Accuracy: 0.4691;Train Accuracy: 0.8658;Validation Accuracy: 0.4700;Train Accuracy: 0.9077;Validation Accuracy: 0.4633;Train Accuracy: 0.9207;Validation Accuracy: 0.4696;Train Accuracy: 0.9377;Validation Accuracy: 0.4900;Train Accuracy: 0.8958;Validation Accuracy: 0.4855;Train Accuracy: 0.9287;Validation Accuracy: 0.4836;Train Accuracy: 0.9379;Validation Accuracy: 0.4636;Train Accuracy: 0.9383;Validation Accuracy: 0.4860;Train Accuracy: 0.9489;Validation Accuracy: 0.4871;Train Accuracy: 0.9297;Validation Accuracy: 0.4760;Train Accuracy: 0.9687;Validation Accuracy: 0.4734;Train Accuracy: 0.9289;Validation Accuracy: 0.4576;Train Accuracy: 0.9561;Validation Accuracy: 0.4874;Train Accuracy: 0.9605;Validation Accuracy: 0.4724;Train Accuracy: 0.9497;Validation Accuracy: 0.4515;Train Accuracy: 0.9605;Validation Accuracy: 0.4627;Train Accuracy: 0.9601;Validation Accuracy: 0.4807;Train Accuracy: 0.9748;Validation Accuracy: 0.4688;Train Accuracy: 0.9649;Validation Accuracy: 0.4740;Train Accuracy: 0.9631;Validation Accuracy: 0.4655;Train Accuracy: 0.9595;Validation Accuracy: 0.4836\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_joint = [\"Joint 1 Block\"]\n",
        "val_joint = [\"Joint 1 Block\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_joint.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_joint.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qbpgFBW2YdpU"
      },
      "outputs": [],
      "source": [
        "# Joint Block2 - 68 min, 2656540\n",
        "\n",
        "temp = \"Train Accuracy: 0.2550;Validation Accuracy: 0.3294;Train Accuracy: 0.3828;Validation Accuracy: 0.4243;Train Accuracy: 0.4986;Validation Accuracy: 0.4185;Train Accuracy: 0.5553;Validation Accuracy: 0.3821;Train Accuracy: 0.6254;Validation Accuracy: 0.4216;Train Accuracy: 0.7250;Validation Accuracy: 0.4181;Train Accuracy: 0.7588;Validation Accuracy: 0.3902;Train Accuracy: 0.7965;Validation Accuracy: 0.4196;Train Accuracy: 0.8387;Validation Accuracy: 0.3960;Train Accuracy: 0.8810;Validation Accuracy: 0.4011;Train Accuracy: 0.8802;Validation Accuracy: 0.3895;Train Accuracy: 0.8986;Validation Accuracy: 0.3641;Train Accuracy: 0.9018;Validation Accuracy: 0.3870;Train Accuracy: 0.9241;Validation Accuracy: 0.3924;Train Accuracy: 0.9313;Validation Accuracy: 0.3971;Train Accuracy: 0.9219;Validation Accuracy: 0.3898;Train Accuracy: 0.8996;Validation Accuracy: 0.3717;Train Accuracy: 0.9113;Validation Accuracy: 0.3785;Train Accuracy: 0.9239;Validation Accuracy: 0.3639;Train Accuracy: 0.9525;Validation Accuracy: 0.3830;Train Accuracy: 0.9545;Validation Accuracy: 0.3876;Train Accuracy: 0.9463;Validation Accuracy: 0.3714;Train Accuracy: 0.9487;Validation Accuracy: 0.3934;Train Accuracy: 0.9541;Validation Accuracy: 0.3991;Train Accuracy: 0.9515;Validation Accuracy: 0.3808;Train Accuracy: 0.9569;Validation Accuracy: 0.3516;Train Accuracy: 0.9471;Validation Accuracy: 0.3650\"\n",
        "\n",
        "splitted = temp.split(\";\")\n",
        "train_joint2 = [\"Joint 2 Block\"]\n",
        "val_joint2 = [\"Joint 2 Block\"]\n",
        "for i in splitted:\n",
        "    if \"Train\" in i:\n",
        "        train_joint2.append(float(i[15:]))\n",
        "    \n",
        "    else:\n",
        "        val_joint2.append(float(i[20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sOfonsXF1_8A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "a = [val_alt, val_2, val_baseline, val_cnn, val_joint, val_perceiver, val_joint2]\n",
        "with open(\"perceiver_validation_acc.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table = [['Model', 'Top-1 Test Accuracy', 'Total Training Time (Minutes)', 'Number of Parameters'], \n",
        "         ['Perceiver Baseline', 0.404, 125, 1625586],\n",
        "         ['Perceiver', 0.431, 50, 301170],\n",
        "         ['Joint (1 Block)', 0.49, 30, 1466518],\n",
        "         ['Joint (2 Block)', 0.4243, 70, 2656540],\n",
        "         ['Alternating (1 Block)', 0.6438, 100, 1693278],\n",
        "         ['Alternating (2 Block)', 0.6146, 300, 3110066],\n",
        "         ['CNN', 0.4574, 33, 480054],\n",
        "         ['ResNet-18', 0.5699, '--', 11689512]]\n",
        "\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ntxAbwz3Jv3s",
        "JT9QP5tx0cSJ",
        "M_9AhFIObbTQ",
        "LhxmtnEdJz1t"
      ],
      "name": "experiments_perceiver_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
